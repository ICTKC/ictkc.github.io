---
title: "Bridging the Gap: Aligning Language Model Generation with Structured Information Extraction via Controllable State Transition"
collection: publications
permalink: /publication/25-www-lihao
excerpt: ''
date: 2025-04-22
venue: ' WWW (CCF-A)'
citation: 'Hao Li, Yubing Ren, Yanan Cao, Yingjie Li, Fang Fang, Zheng Lin, Shi Wang: Bridging the Gap: Aligning Language Model Generation with Structured Information Extraction via Controllable State Transition. WWW 2025: 1811-1821'
---
Abstract
--
Large language models (LLMs) achieve superior performance in generative tasks. However, due to the natural gap between language model generation and structured information extraction in three dimensions: task type, output format, and modeling granularity, they often fall short in structured information extraction, a crucial capability for effective data utilization on the web. In this paper, we define the generation process of the language model as the controllable state transition, aligning the generation and extraction processes to ensure the integrity of the output structure and adapt to the goals of the information extraction task. Furthermore, we propose the Structure2Text decider to help the language model understand the fine-grained extraction information, which converts the structured output into natural language and makes state decisions, thereby focusing on the task-specific information kernels, and alleviating language model hallucinations and incorrect content generation. We conduct extensive experiments and detailed analyses on myriad information extraction tasks, including named entity recognition, relation extraction, and event argument extraction. Our method not only achieves significant performance improvements but also considerably enhances the model's capability to generate precise and relevant content, making the extracted content easy to parse.

[Download](https://dl.acm.org/doi/10.1145/3696410.3714571)
